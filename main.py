import asyncio
from datetime import datetime, timedelta, UTC
from pathlib import Path

import httpx
import feedparser
from dateutil import parser as date_parser

from models import FeedItem, Feed, Author
from db import Database
from recipes import get_recipes
from filters import should_include_item
from utils import logger


async def fetch_feed(client: httpx.AsyncClient, url: str) -> str:
    """Fetch feed content from URL."""
    logger.info(f"Fetching feed from {url}")
    response = await client.get(url)
    response.raise_for_status()
    return response.text


def parse_date(date_str: str) -> datetime:
    """Parse date string to UTC datetime."""
    try:
        return date_parser.parse(date_str).astimezone(UTC)
    except (ValueError, TypeError) as e:
        logger.warning(f"Failed to parse date '{date_str}': {e}")
        return datetime.now(UTC)


async def process_feed(
    client: httpx.AsyncClient, db: Database, feed_name: str, feed_config: dict
) -> list[FeedItem]:
    """Process a feed configuration and return feed items."""
    items = []
    week_ago = datetime.now(UTC) - timedelta(days=7)
    
    for url in feed_config["urls"]:
        try:
            content = await fetch_feed(client, url)
            feed = feedparser.parse(content)
            logger.info(f"Processing {len(feed.entries)} entries from {url}")
            
            for entry in feed.entries:
                published = parse_date(entry.get("published", ""))
                if published < week_ago:
                    logger.debug(f"Skipping old entry from {published}")
                    continue
                
                if not should_include_item(entry, feed_config.get("filters", [])):
                    continue
                
                # Extract author information if available
                author = None
                if entry.get("author_detail"):
                    author = Author(
                        name=entry.get("author_detail", {}).get("name"),
                        url=entry.get("author_detail", {}).get("href"),
                    )
                
                item = FeedItem(
                    id=entry.link,
                    url=entry.link,
                    title=entry.title,
                    content_text=entry.get("summary"),
                    content_html=entry.get("content", [{}])[0].get("value"),
                    summary=entry.get("summary"),
                    date_published=published,
                    author=author,
                    tags=entry.get("tags", []),
                )
                
                await db.upsert_item(item)
                items.append(item)
                
        except Exception as e:
            logger.error(f"Error processing {url}: {e}", exc_info=True)
            
    return items


async def main():
    logger.info("Starting feed aggregation")
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    db = Database()
    await db.init()

    # Configure client with timeouts and redirect handling
    async with httpx.AsyncClient(
        timeout=30.0,  # 30 seconds timeout
        follow_redirects=True,
        max_redirects=5,
    ) as client:
        for feed_name, feed_config in get_recipes().items():
            logger.info(f"Processing feed: {feed_name}")
            items = await process_feed(client, db, feed_name, feed_config)

            feed = Feed(
                title=feed_name,
                items=sorted(items, key=lambda x: x.date_published, reverse=True),
                description=f"Aggregated feed for {feed_name}",
                user_comment="Generated by FeedForger",
            )

            output_path = output_dir / f"{feed_name}.json"
            output_path.write_text(feed.model_dump_json(indent=2))
            logger.info(f"Generated feed file: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())
